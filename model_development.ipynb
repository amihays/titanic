{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "(steps from [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurélien Géron, Appendix B)\n",
    "\n",
    "- Data exploration\n",
    "    1. Study each attribute and its characteristics\n",
    "        - Name\n",
    "        - Type\n",
    "        - % of missing values\n",
    "        - Noisiness and type of noise (stochastic, outliers, rounding errors, etc)\n",
    "        - Usefulness for the task\n",
    "        - Type of distribution\n",
    "    2. Identify target attribute(s)\n",
    "    3. Visualize the data\n",
    "    4. Study correlations between attributes\n",
    "    5. Study how you'd solve the problem manually\n",
    "    6. Identify promising transformations\n",
    "    7. Document\n",
    "- Data preparation\n",
    "    - Work on copies of data (keep original dataset intact)\n",
    "    - Write functions for transformations with preparation choices as hyperparameters\n",
    "    1. Data cleaning\n",
    "        - Fix or remove outliers (opt)\n",
    "        - Fill in missing values or drop their rows\n",
    "    2. Feature selection\n",
    "        - Drop attributes that aren't useful\n",
    "    3. Feature engineering\n",
    "        - Discretize continuous features (doesn't seem helpful here)\n",
    "        - Decompose features (categorical => numerical)\n",
    "        - Add promising transformations (ex: log(x), sqrt(x), etc)\n",
    "        - Aggregate features into promising new features\n",
    "    4. Feature scaling\n",
    "        - Standardize or normalize features\n",
    "- Shortlist promising models\n",
    "    - Once again, try to automate\n",
    "    1. Train many quick-and-dirty models from different categories with standard parameters\n",
    "    2. Measure and compare their performance\n",
    "        - Use k-fold cross-validation for each, getting mean and standard deviation\n",
    "    3. Analyze the most significant variables for each algorithm\n",
    "    4. Analyze the types of errors the models make\n",
    "        - What data would a human use to avoid these errors?\n",
    "    5. Perform a quick round of feature selection and engineering\n",
    "    6. Perform 1-2 more quick iterations of steps 1-5\n",
    "    7. Shortlist the top 3-5 most promising models, preferring models that make different types of errors\n",
    "- Fine-tune the system\n",
    "    - Use as much data as possible, especially as you move toward the end of fine-tuning\n",
    "    - Automate when possible\n",
    "    1. Fine-tune the hyperparameters using cross-validation\n",
    "        - Treat data transformation choices as hyperparameters, esp when not sure about them\n",
    "        - Unless there are very few hyperparameter values to explore, prefer random search over grid search\n",
    "    2. Try Ensemble methods. Combining best models will often improve performance over individual models.\n",
    "    3. Once confident about final model, measure performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
